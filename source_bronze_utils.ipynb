{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845ed104-908a-4e4a-a4d2-534bd68ca627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, lower, upper, when, regexp_replace\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Convert column names to lowercase and replace spaces with underscores\"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumnRenamed(col_name, col_name.lower().replace(\" \", \"_\"))\n",
    "    return df\n",
    "\n",
    "def trim_string_columns(df):\n",
    "    \"\"\"Trim all string columns\"\"\"\n",
    "    string_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() == 'string']\n",
    "    for c in string_cols:\n",
    "        df = df.withColumn(c, trim(col(c)))\n",
    "    return df\n",
    "\n",
    "def remove_special_chars(df, column):\n",
    "    \"\"\"Remove special characters from a column\"\"\"\n",
    "    return df.withColumn(column, regexp_replace(col(column), \"[^a-zA-Z0-9 ]\", \"\"))\n",
    "\n",
    "def convert_to_lower(df, column):\n",
    "    return df.withColumn(column, lower(col(column)))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "source_bronze_utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}