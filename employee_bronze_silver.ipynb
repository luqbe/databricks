{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3b0a52-62cc-479c-95e5-f41c847ce37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../source_bronze/source_bronze_utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc5febb3-d8d7-412c-a4fd-3971f0971f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b0bc13-bc19-43af-ae91-06a9bcc22bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Empty table `employee_info_abu.dim_employee` created at /FileStore/silver/employee/employee_info_abu/dim_employee\n"
     ]
    }
   ],
   "source": [
    "## CREATE TABLE\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Define snake_case schema for the employee table\n",
    "employee_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"employee_name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"load_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Create empty DataFrame with schema\n",
    "df_employee = spark.createDataFrame([], schema=employee_schema)\n",
    "\n",
    "# Set database, table name, and location\n",
    "db_name = \"employee_info_abu\"\n",
    "table_name = \"dim_employee\"\n",
    "table_path = f\"/FileStore/silver/employee/{db_name}/{table_name}\"\n",
    "\n",
    "# Create the database if not exists\n",
    "spark.sql(f\"CREATE schema IF NOT EXISTS {db_name}\")\n",
    "\n",
    "# Create the table only if it doesn't exist\n",
    "if not spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\"):\n",
    "    df_employee.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {db_name}.{table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{table_path}'\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"✅ Empty table `{db_name}.{table_name}` created at {table_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623307e5-72b8-4fdd-9fc2-c76b0f1cfcf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns after rename: ['employee_id', 'employee_name', 'department', 'country', 'salary', 'age']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col, current_date\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# Function to convert camelCase to snake_case\n",
    "def camel_to_snake(name):\n",
    "    name = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name)\n",
    "    return name.lower()\n",
    "\n",
    "def convert_column_names_to_snake_case(df):\n",
    "    new_col_names = [camel_to_snake(col) for col in df.columns]\n",
    "    for old, new in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(old, new)\n",
    "    return df\n",
    "\n",
    "# 1. Read source data\n",
    "df_employee = spark.read.option(\"header\", True).csv(\"/FileStore/source_to_bronze/employee\")\n",
    "\n",
    "# 2. Convert column names to snake_case\n",
    "df_employee = convert_column_names_to_snake_case(df_employee)\n",
    "\n",
    "# 3. Check your column names (optional but useful)\n",
    "print(\"✅ Columns after rename:\", df_employee.columns)\n",
    "\n",
    "# 4. Cast to match target table schema\n",
    "df_employee = df_employee.select(\n",
    "    col(\"employee_id\").cast(IntegerType()),\n",
    "    col(\"employee_name\").cast(StringType()),\n",
    "    col(\"department\").cast(StringType()),\n",
    "    col(\"country\").cast(StringType()),\n",
    "    col(\"salary\").cast(IntegerType()),\n",
    "    col(\"age\").cast(IntegerType())\n",
    ")\n",
    "\n",
    "# 5. Add load_date and remove duplicates\n",
    "df_employee = df_employee.withColumn(\"load_date\", current_date())\n",
    "df_employee = df_employee.dropDuplicates([\"employee_id\"])\n",
    "\n",
    "# 6. Append to Delta table\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employee_info_abu.dim_employee\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ec70a7-bddb-42f1-8057-63666ab63ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>employee_name</th><th>department</th><th>country</th><th>salary</th><th>age</th><th>load_date</th></tr></thead><tbody><tr><td>1</td><td>James</td><td>D101</td><td>IN</td><td>9000</td><td>25</td><td>2025-04-19</td></tr><tr><td>2</td><td>Michel</td><td>D102</td><td>SA</td><td>8000</td><td>26</td><td>2025-04-19</td></tr><tr><td>3</td><td>James son</td><td>D101</td><td>IN</td><td>10000</td><td>35</td><td>2025-04-19</td></tr><tr><td>4</td><td>Robert</td><td>D103</td><td>MY</td><td>11000</td><td>34</td><td>2025-04-19</td></tr><tr><td>5</td><td>Scott</td><td>D104</td><td>MA</td><td>6000</td><td>36</td><td>2025-04-19</td></tr><tr><td>6</td><td>Gen</td><td>D105</td><td>JA</td><td>21345</td><td>24</td><td>2025-04-19</td></tr><tr><td>7</td><td>John</td><td>D102</td><td>MY</td><td>87654</td><td>40</td><td>2025-04-19</td></tr><tr><td>8</td><td>Maria</td><td>D105</td><td>SA</td><td>38144</td><td>38</td><td>2025-04-19</td></tr><tr><td>9</td><td>Soffy</td><td>D103</td><td>IN</td><td>23456</td><td>29</td><td>2025-04-19</td></tr><tr><td>10</td><td>Amy</td><td>D103</td><td>CN</td><td>21345</td><td>24</td><td>2025-04-19</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "James",
         "D101",
         "IN",
         9000,
         25,
         "2025-04-19"
        ],
        [
         2,
         "Michel",
         "D102",
         "SA",
         8000,
         26,
         "2025-04-19"
        ],
        [
         3,
         "James son",
         "D101",
         "IN",
         10000,
         35,
         "2025-04-19"
        ],
        [
         4,
         "Robert",
         "D103",
         "MY",
         11000,
         34,
         "2025-04-19"
        ],
        [
         5,
         "Scott",
         "D104",
         "MA",
         6000,
         36,
         "2025-04-19"
        ],
        [
         6,
         "Gen",
         "D105",
         "JA",
         21345,
         24,
         "2025-04-19"
        ],
        [
         7,
         "John",
         "D102",
         "MY",
         87654,
         40,
         "2025-04-19"
        ],
        [
         8,
         "Maria",
         "D105",
         "SA",
         38144,
         38,
         "2025-04-19"
        ],
        [
         9,
         "Soffy",
         "D103",
         "IN",
         23456,
         29,
         "2025-04-19"
        ],
        [
         10,
         "Amy",
         "D103",
         "CN",
         21345,
         24,
         "2025-04-19"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "load_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from employee_info_abu.dim_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae3f68d-b00f-48b3-b33e-dcfe4d7c3888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Empty table `employee_info_abu.dim_depart` created at /FileStore/silver/department/employee_info_abu/dim_depart\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "depart_schema = StructType([\n",
    "    StructField(\"department_id\", StringType(), True),\n",
    "    StructField(\"department_name\", StringType(), True),\n",
    "    StructField(\"load_date\", DateType(), True)\n",
    "])\n",
    "df_depart = spark.createDataFrame([], schema=depart_schema)\n",
    "\n",
    "db_name = \"employee_info_abu\"\n",
    "table_name = \"dim_depart\"\n",
    "table_path = f\"/FileStore/silver/department/{db_name}/{table_name}\"\n",
    "\n",
    "spark.sql(f\"CREATE schema IF NOT EXISTS {db_name}\")\n",
    "if not spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\"):\n",
    "    df_depart.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {db_name}.{table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{table_path}'\n",
    "    \"\"\")\n",
    "    print(f\"✅ Empty table `{db_name}.{table_name}` created at {table_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b03d04f-841b-470b-a4c9-3fc46de0f1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns after rename: ['department_id', 'department_name']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col, current_date\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# Function to convert camelCase to snake_case\n",
    "def camel_to_snake(name):\n",
    "    name = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name)\n",
    "    return name.lower()\n",
    "\n",
    "def convert_column_names_to_snake_case(df):\n",
    "    new_col_names = [camel_to_snake(col) for col in df.columns]\n",
    "    for old, new in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(old, new)\n",
    "    return df\n",
    "\n",
    "# 1. Read source data\n",
    "df_depart = spark.read.option(\"header\", True).csv(\"/FileStore/source_to_bronze/department_df\")\n",
    "\n",
    "# 2. Convert column names to snake_case\n",
    "df_depart = convert_column_names_to_snake_case(df_depart)\n",
    "\n",
    "# 3. Check your column names (optional but useful)\n",
    "print(\"✅ Columns after rename:\", df_depart.columns)\n",
    "\n",
    "# 4. Cast to match target table schema\n",
    "df_depart = df_depart.select(\n",
    "    col(\"department_id\").cast(StringType()),\n",
    "    col(\"department_name\").cast(StringType())\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# 5. Add load_date and remove duplicates\n",
    "df_depart = df_depart.withColumn(\"load_date\", current_date())\n",
    "\n",
    "\n",
    "# 6. Append to Delta table\n",
    "df_depart.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"employee_info_abu.dim_depart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fc5b2f-c3fb-4476-af62-236e399fc970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department_id</th><th>department_name</th><th>load_date</th></tr></thead><tbody><tr><td>D101</td><td>Sales</td><td>2025-04-19</td></tr><tr><td>D102</td><td>Marketing</td><td>2025-04-19</td></tr><tr><td>D103</td><td>Finance</td><td>2025-04-19</td></tr><tr><td>D104</td><td>Support</td><td>2025-04-19</td></tr><tr><td>D105</td><td>HR</td><td>2025-04-19</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "D101",
         "Sales",
         "2025-04-19"
        ],
        [
         "D102",
         "Marketing",
         "2025-04-19"
        ],
        [
         "D103",
         "Finance",
         "2025-04-19"
        ],
        [
         "D104",
         "Support",
         "2025-04-19"
        ],
        [
         "D105",
         "HR",
         "2025-04-19"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "department_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "load_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from employee_info_abu.dim_depart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f534bb2-fbc4-4250-86d7-9db079e55106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Empty table `employee_info_abu.dim_country` created at /FileStore/silver/country/employee_info_abu/dim_country\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "country_schema = StructType([\n",
    "    StructField(\"country_code\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"load_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "df_country = spark.createDataFrame([], schema=country_schema)\n",
    "\n",
    "db_name = \"employee_info_abu\"\n",
    "table_name = \"dim_country\"\n",
    "table_path = f\"/FileStore/silver/country/{db_name}/{table_name}\"\n",
    "\n",
    "spark.sql(f\"CREATE schema IF NOT EXISTS {db_name}\")\n",
    "if not spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\"):\n",
    "    df_country.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(table_path)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {db_name}.{table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{table_path}'\n",
    "    \"\"\")\n",
    "    print(f\"✅ Empty table `{db_name}.{table_name}` created at {table_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c19ec58-9260-478a-8d96-ea7cc99e65cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns after rename: ['country_code', 'country_name']\n+------------+------------+\n|country_code|country_name|\n+------------+------------+\n|          CN|       China|\n|          IN|       India|\n|          SA|South Africa|\n|          JA|       Japan|\n|          MY|    Malaysia|\n|          MA|     Morocco|\n+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to convert camelCase to snake_case\n",
    "def camel_to_snake(name):\n",
    "    name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name)  # Captures lowercase followed by uppercase\n",
    "    return name.lower()\n",
    "\n",
    "def convert_column_names_to_snake_case(df):\n",
    "    new_col_names = [camel_to_snake(col) for col in df.columns]\n",
    "    for old, new in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(old, new)\n",
    "    return df\n",
    "\n",
    "# 1. Read source data\n",
    "df_country = spark.read.option(\"header\", True).csv(\"/FileStore/source_to_bronze/country_df\")\n",
    "\n",
    "# 2. Convert column names to snake_case (renaming camelCase columns)\n",
    "df_country = convert_column_names_to_snake_case(df_country)\n",
    "df_country = df_country.withColumnRenamed(\"countrycode\", \"country_code\") \\\n",
    "                       .withColumnRenamed(\"countryname\", \"country_name\")\n",
    "# 3. Check the new column names\n",
    "print(\"✅ Columns after rename:\", df_country.columns)\n",
    "\n",
    "# 4. Cast to match target table schema (now that the columns are in snake_case)\n",
    "df_country = df_country.select(\n",
    "    col(\"country_code\").cast(StringType()),  # Should match the new column names\n",
    "    col(\"country_name\").cast(StringType())   # Should match the new column names\n",
    ")\n",
    "\n",
    "# 5. Show the dataframe (optional for debugging)\n",
    "df_country.show()\n",
    "df_country.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(\"employee_info_abu.dim_country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a13344aa-bf3f-41d7-a83e-e1b2ff8c0784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from employee_info_abu.dim_country"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4013470113192177,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "employee_bronze_silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}